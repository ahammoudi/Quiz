Question #: 1
An application requires a storage solution that can be mounted by hundreds of EC2 instances simultaneously and provides a hierarchical file system for shared access. Which storage service is the most appropriate?

A. Amazon EBS
B. Amazon EFS
C. Amazon S3
D. Amazon Instance Store

Answer: B

Explanation: Amazon EFS (Elastic File System) is a fully managed, scalable file storage service designed to be mounted by thousands of EC2 instances concurrently, making it ideal for shared file access. EBS volumes (A) are block storage that can typically only be attached to a single instance. S3 (C) is object storage and cannot be mounted as a file system without a file gateway. Instance Store (D) is ephemeral storage local to a single instance.

---

Question #: 2
A high-performance computing (HPC) workload requires an EBS volume with the highest possible IOPS and lowest latency for a critical database. Which EBS volume type should be used?

A. Provisioned IOPS SSD (io2 Block Express)
B. General Purpose SSD (gp3)
C. Throughput Optimized HDD (st1)
D. Cold HDD (sc1)

Answer: A

Explanation: The io2 Block Express volume type is the highest-performance EBS storage, designed for I/O-intensive, mission-critical applications. It offers the highest IOPS, throughput, and lowest latency of all EBS volume types. gp3 (B) offers a balance of price and performance, while st1 (C) and sc1 (D) are HDD-based and optimized for throughput and low cost, respectively, not high IOPS.

---

Question #: 3
You need to select an EC2 instance type optimized for a workload that involves large-scale data processing and requires high sequential read/write access to large datasets on local storage. Which instance family is the best choice?

A. T-family (e.g., t3.large)
B. C-family (e.g., c5.xlarge)
C. I-family (e.g., i3.xlarge)
D. G-family (e.g., g4dn.xlarge)

Answer: C

Explanation: The I-family of instances is storage-optimized. They are designed for workloads that require high, sequential read and write access to very large data sets on local storage. They provide high-performance, low-latency NVMe SSD instance storage, which is perfect for this use case. T-family (A) is for burstable workloads, C-family (B) is compute-optimized, and G-family (D) is for GPU workloads.

---

Question #: 4
An application is designed to upload and download millions of small files to and from an S3 bucket. What is a key performance consideration for this workload?

A. The S3 bucket's total storage size.
B. The AWS region the bucket is in.
C. S3 can handle a very high number of requests per second, but performance can be optimized by using multiple prefixes in the bucket.
D. The S3 storage class (Standard vs. Glacier).

Answer: C

Explanation: Amazon S3 automatically scales to handle a very high request rate. It partitions its performance based on object prefixes. By spreading your uploads and downloads across multiple distinct prefixes (which act like folders), you can leverage more parallel request processing capabilities, significantly increasing the overall request rate your application can achieve.

---

Question #: 5
You are using a gp3 EBS volume and need to increase its IOPS without changing the volume size. Is this possible, and what is the benefit of gp3 in this scenario?

A. No, IOPS are directly tied to the size of a gp3 volume.
B. Yes, gp3 allows you to provision IOPS and throughput independently of the storage size, providing more flexibility and cost savings.
C. Yes, but you must also increase the throughput proportionally.
D. No, only io2 volumes allow for independent IOPS provisioning.

Answer: B

Explanation: The primary advantage of the gp3 volume type is that it decouples performance from storage size. Unlike its predecessor gp2, you can increase the IOPS and throughput of a gp3 volume without having to increase the volume's size, which allows you to pay only for the performance you need without over-provisioning storage.

---

Question #: 6
A distributed data processing application requires a group of EC2 instances to have high-bandwidth, low-latency network connectivity between them. Which feature should be used?

A. A VPC Peering connection.
B. An Elastic Fabric Adapter (EFA).
C. A Placement Group with a "cluster" strategy.
D. An Internet Gateway.

Answer: C

Explanation: A cluster placement group is a logical grouping of instances within a single Availability Zone. It is designed to pack instances closely together on the underlying hardware to provide low-latency, high-throughput networking between them. This is ideal for tightly coupled high-performance computing (HPC) workloads.

---

Question #: 7
An application reads and writes to an Amazon EFS file system from a large fleet of EC2 instances. The workload involves many small, metadata-heavy file operations. The application is experiencing higher-than-expected latency. Which EFS performance mode should be used to optimize for this scenario?

A. General Purpose mode
B. Max I/O mode
C. Provisioned Throughput mode
D. Bursting Throughput mode

Answer: A

Explanation: EFS offers two performance modes. General Purpose mode is the default and is optimized for latency-sensitive use cases, such as content management systems, web serving, and development workflows, which typically involve many small file operations. Max I/O mode (B) is optimized for higher aggregate throughput and higher levels of parallel operations but has slightly higher latency for metadata operations.

---

Question #: 8
To improve the performance of large object uploads to S3, which strategy should be used?

A. Uploading the file in a single, large PUT operation.
B. Using S3 Transfer Acceleration.
C. Using Multipart Upload to break the large object into smaller parts and upload them in parallel.
D. Placing the object in the S3 One Zone-IA storage class.

Answer: C

Explanation: Multipart Upload is the recommended method for uploading large files (anything over 100 MB). It allows you to upload a single object as a set of parts. These parts can be uploaded independently and in parallel, which can significantly increase throughput by better utilizing your available network bandwidth. It also provides resiliency, as a failed part can be re-uploaded without restarting the entire object upload.

---

Question #: 9
What is an "EBS-optimized instance"?

A. An EC2 instance that can only use EBS volumes and not instance store.
B. An EC2 instance that provides a dedicated, optimized network connection between the instance and its EBS volumes.
C. An EC2 instance that automatically encrypts all attached EBS volumes.
D. An EC2 instance that uses a special, high-performance type of EBS volume.

Answer: B

Explanation: Standard EC2 instances share network bandwidth between regular network traffic and EBS I/O traffic. An EBS-optimized instance provides a dedicated, high-bandwidth connection exclusively for traffic to and from its EBS volumes. This minimizes contention and provides the best and most consistent performance for I/O-intensive workloads. Most modern instance types are EBS-optimized by default.

---

Question #: 10
A database running on an EC2 instance requires a minimum of 30,000 IOPS at all times. The database size is only 200 GiB. Which EBS configuration meets this requirement in the most cost-effective way?

A. A 10,000 GiB gp2 volume.
B. A 200 GiB gp3 volume with 30,000 IOPS provisioned.
C. A 200 GiB st1 volume.
D. A 200 GiB io2 volume with 30,000 IOPS provisioned.

Answer: D

Explanation: The gp3 volume type (B) allows provisioning IOPS, but it has a maximum of 16,000 IOPS, which is not enough. The io2 volume type is designed for high-performance workloads and can be provisioned with up to 64,000 IOPS per volume. Provisioning a 200 GiB io2 volume with the required 30,000 IOPS directly meets the requirement without over-provisioning storage. A gp2 volume (A) would need to be very large (and expensive) to achieve this IOPS level.

---

Question #: 11
Which EC2 purchasing option is most suitable for a short-term, stateless, high-performance computing job that is fault-tolerant and can be interrupted?

A. On-Demand Instances
B. Reserved Instances
C. Spot Instances
D. Dedicated Hosts

Answer: C

Explanation: Spot Instances allow you to bid on spare EC2 compute capacity at a significant discount compared to On-Demand prices. They are ideal for fault-tolerant, flexible workloads because AWS can reclaim the instance with a two-minute warning. For a batch HPC job that can be stopped and restarted, this provides the best price-performance ratio.

---

Question #: 12
An Amazon EFS file system is configured with Bursting Throughput mode. What determines its baseline throughput?

A. The number of EC2 instances connected to it.
B. The amount of data stored in the EFS Standard storage class.
C. A value that is manually configured by the user.
D. The AWS Region it is located in.

Answer: B

Explanation: In Bursting Throughput mode, the baseline performance of an EFS file system is directly proportional to its size. Specifically, you get 50 KiB/s of baseline throughput for every 1 GiB of data stored in the EFS Standard storage class. The file system accrues burst credits when it is idle, which it can then use to burst to higher throughput levels.

---

Question #: 13
An application needs to frequently read a 500 GB dataset from an S3 bucket for analysis. To optimize performance for repeated reads, what is a good strategy?

A. Store the dataset in the S3 Glacier Deep Archive storage class.
B. Use a Placement Group for the EC2 instances reading the data.
C. Copy the data from S3 to a high-throughput EBS volume (like st1) or instance store on the EC2 instance before processing.
D. Increase the number of prefixes in the S3 bucket.

Answer: C

Explanation: While S3 has high throughput, reading data over the network is always slower than reading from local storage. For repeated analysis of the same dataset, the best performance will be achieved by first copying the data from S3 to a high-performance local storage option on the EC2 instance (like an st1 EBS volume for high sequential throughput, or a local NVMe instance store). The analysis can then run against the local copy, maximizing I/O performance.

---

Question #: 14
What is the primary characteristic of an EC2 "burstable performance" instance (T-family)?

A. It provides a baseline level of CPU performance and the ability to burst to a higher level for a limited time.
B. It has the highest network bandwidth of all instance types.
C. It is optimized for memory-intensive applications.
D. It provides a dedicated physical server.

Answer: A

Explanation: Burstable performance instances (like T2, T3, T4g) are designed for workloads that are typically idle or have low CPU usage but occasionally need to burst to full CPU power. They accrue "CPU credits" when they are idle, which they can then spend to burst above their baseline performance when the application needs it. This provides a very cost-effective option for many general-purpose workloads.

---

Question #: 15
An application running on EC2 requires a shared file system. The application needs the highest possible IOPS and lowest latency for its file operations. Which configuration should be chosen?

A. Amazon EFS in General Purpose performance mode.
B. Amazon EFS in Max I/O performance mode.
C. Amazon FSx for Lustre.
D. Amazon EBS Multi-Attach.

Answer: C

Explanation: Amazon FSx for Lustre is a high-performance file system designed for the fastest processing of workloads. It is ideal for HPC, machine learning, and media processing workloads that require a POSIX-compliant file system with extremely high throughput and sub-millisecond latencies. It generally offers higher performance than Amazon EFS for these types of parallel workloads.

---

Question #: 16
What is the purpose of S3 Transfer Acceleration?

A. To encrypt data in transit to S3.
B. To provide a static IP address for an S3 bucket.
C. To enable fast, easy, and secure transfers of files over long distances by using CloudFront's globally distributed edge locations.
D. To accelerate the processing of data once it is in S3.

Answer: C

Explanation: S3 Transfer Acceleration is designed to speed up large object uploads from geographically dispersed clients. Instead of uploading directly to the S3 bucket's regional endpoint, users upload to a nearby CloudFront edge location. The data then travels over the highly optimized AWS global network to the S3 bucket, which is often much faster and more reliable than traversing the public internet.

---

Question #: 17
You are launching a fleet of EC2 instances that need to communicate with each other in a full mesh network. You need to ensure that if one instance fails, it does not affect the others. Which placement group strategy is appropriate?

A. Cluster
B. Spread
C. Partition
D. None, placement groups are not suitable for this.

Answer: B

Explanation: A Spread placement group is designed for high availability. It places a small number of critical instances on distinct underlying hardware racks. This ensures that if a single piece of hardware fails (like a rack power supply or network switch), it will only impact a single instance in your group. This is ideal for applications like database masters or cluster managers.

---

Question #: 18
Which EBS volume type is designed as the lowest-cost option for large, sequential workloads like big data processing, data warehouses, and log processing?

A. General Purpose SSD (gp3)
B. Provisioned IOPS SSD (io2)
C. Throughput Optimized HDD (st1)
D. Cold HDD (sc1)

Answer: C

Explanation: Throughput Optimized HDD (st1) volumes are backed by hard disk drives and are designed to provide low-cost, high-throughput performance. They are ideal for large, sequential I/O operations, which is the access pattern for data warehousing and log processing. They are not suitable for workloads that require high random-access IOPS, like databases.

---

Question #: 19
An EC2 instance is launched with an instance store volume. What happens to the data on the instance store volume if the instance is stopped and then started again?

A. The data is persisted and will be available when the instance starts.
B. The data is moved to an EBS snapshot.
C. The data is permanently lost.
D. The data is encrypted by default.

Answer: C

Explanation: Instance store provides ephemeral, temporary block-level storage. The data on an instance store volume persists only for the life of that instance. If the instance is stopped, hibernated, or terminated, all data on its instance store volumes is erased. This is why it is only suitable for temporary data.

---

Question #: 20
To achieve the absolute maximum network performance for tightly coupled HPC applications on EC2, what feature should be used?

A. Enhanced Networking with the ENA driver.
B. An Elastic Fabric Adapter (EFA).
C. A 100 Gbps network connection.
D. A cluster placement group.

Answer: B

Explanation: An Elastic Fabric Adapter (EFA) is a network interface for EC2 instances that is purpose-built for HPC and machine learning applications. It provides lower and more consistent latency and higher throughput than traditional TCP transport by using the Scalable Reliable Datagram protocol and OS-bypass capabilities. It is a step above standard Enhanced Networking (A) and is often used in conjunction with cluster placement groups (D).

---

Question #: 21
What is a primary benefit of using Amazon S3 for storing large, infrequently accessed data archives instead of an EBS volume?

A. S3 provides lower latency access.
B. S3 offers virtually unlimited scalability and is significantly more cost-effective for archival storage.
C. S3 data can be mounted as a local file system.
D. S3 data is replicated across multiple regions by default.

Answer: B

Explanation: S3 is an object storage service designed for massive scale and durability at a very low cost. For archival data, you can use storage classes like S3 Glacier and Glacier Deep Archive, which have an extremely low cost per GB. An EBS volume of the same size would be far more expensive and is not designed for unlimited scalability.

---

Question #: 22
Which of the following is a characteristic of a Provisioned IOPS SSD (io2) EBS volume?

A. It is the lowest-cost EBS volume type.
B. Its performance is measured primarily in MB/s of throughput.
C. It is designed to deliver a consistent, provisioned number of IOPS with low latency.
D. Its IOPS performance scales linearly with its size.

Answer: C

Explanation: The key feature of io2 (and its predecessor io1) volumes is performance consistency. They are designed for I/O-intensive workloads, like transactional databases, that require a sustained and predictable level of IOPS. You provision the exact number of IOPS you need, and the service is designed to deliver that performance consistently.

---

Question #: 23
A media company needs a shared storage solution for its video editing workflow. Multiple Mac and Windows workstations need to access and modify the same video files simultaneously. The files need to be accessible from their on-premises network. Which service is the best fit?

A. Amazon EFS
B. Amazon S3
C. Amazon FSx for Windows File Server
D. Amazon EBS

Answer: C

Explanation: Amazon FSx for Windows File Server provides a fully managed, native Windows file system. It is ideal for this use case because it supports the SMB protocol, which is native to Windows and macOS, and it provides the features, performance, and compatibility of a Windows file server. It can also be accessed from on-premises environments. EFS (A) uses the NFS protocol, which is less native for Windows.

---

Question #: 24
To get the best possible networking performance on a supported EC2 instance, what must be enabled?

A. Detailed Monitoring
B. A placement group
C. An Elastic IP address
D. Enhanced Networking

Answer: D

Explanation: Enhanced Networking uses single root I/O virtualization (SR-IOV) to provide high-performance networking capabilities on supported instance types. It results in higher packets per second (PPS), lower inter-instance latency, and lower network jitter. It is enabled by using drivers like the Elastic Network Adapter (ENA).

---

Question #: 25
When considering S3 performance, what is a "prefix"?

A. The first part of an object key, delimited by a slash (e.g., `logs/`).
B. A special tag that increases an object's request rate.
C. The S3 bucket name.
D. The AWS region identifier.

Answer: A

Explanation: In S3, an object key is the full path to the object (e.g., `logs/2023/08/event.json`). A prefix is any string of characters from the beginning of the key up to a delimiter. For performance purposes, S3 partitions its index based on these prefixes. Spreading your objects across many different prefixes allows S3 to scale its request handling capabilities in parallel.

---

Question #: 26
An application requires a minimum of 5,000 IOPS from its 500 GiB EBS volume. Which configuration is valid and cost-effective?

A. A 500 GiB gp2 volume.
B. A 500 GiB gp3 volume with 5,000 IOPS provisioned.
C. A 500 GiB st1 volume.
D. A 1667 GiB gp2 volume.

Answer: B

Explanation: A gp2 volume's IOPS are calculated as 3 IOPS per GiB. A 500 GiB gp2 volume (A) would only provide 1,500 IOPS (3 * 500), which is not enough. To get 5,000 IOPS from gp2, you would need a 1667 GiB volume (D), which is not cost-effective as you are paying for storage you don't need. A gp3 volume (B) allows you to provision the required 5,000 IOPS directly on the 500 GiB volume, making it the most cost-effective choice.

---

Question #: 27
What is the primary difference between Amazon EFS and Amazon FSx for Lustre?

A. EFS is for Windows, and FSx for Lustre is for Linux.
B. EFS is a general-purpose, scalable file system, while FSx for Lustre is a high-performance file system optimized for HPC and compute-intensive workloads.
C. EFS is object storage, while FSx for Lustre is file storage.
D. EFS is less expensive than FSx for Lustre for all workloads.

Answer: B

Explanation: While both are file systems, they are optimized for different use cases. EFS is designed for a broad range of general-purpose workloads and scales its performance automatically. FSx for Lustre is a specialized, parallel file system that provides the highest levels of throughput and IOPS for demanding HPC, AI/ML, and media processing workloads.

---

Question #: 28
Which of the following instance types are optimized for compute-intensive workloads like high-performance web servers, scientific modeling, and batch processing?

A. R-family (Memory Optimized)
B. T-family (Burstable)
C. C-family (Compute Optimized)
D. I-family (Storage Optimized)

Answer: C

Explanation: The C-family of instances (e.g., c5, c6g) are designed to provide the best price-performance for compute-bound applications. They have a high ratio of CPU power to memory, making them ideal for workloads that require significant processing power.

---

Question #: 29
You are uploading a 20 GB file to S3 using a single PUT operation from an EC2 instance in the same region. The upload is slow. What is the most likely bottleneck?

A. The S3 service is being throttled.
B. The network throughput of the EC2 instance is limited.
C. The EBS volume on the EC2 instance is too slow.
D. You are not using Multipart Upload.

Answer: B

Explanation: While not using Multipart Upload (D) is inefficient, the actual bottleneck for a single stream is often the network bandwidth allocated to the EC2 instance itself. Different instance types have different levels of network performance. If you are on a small instance type, its maximum network throughput may be the limiting factor for your upload speed.

---

Question #: 30
A company needs to host a large relational database that requires 100,000 IOPS and 2,000 MB/s of throughput. They want to use a managed storage solution. What should they use?

A. An EC2 instance with a local instance store.
B. An Amazon EBS io2 Block Express volume.
C. An Amazon EFS file system in Max I/O mode.
D. An S3 bucket.

Answer: B

Explanation: These performance requirements (very high IOPS and throughput) are characteristic of a high-end transactional database. The io2 Block Express volume type is specifically designed to meet these needs, supporting up to 256,000 IOPS and 4,000 MB/s of throughput per volume.

---

Question #: 31
You have a fleet of instances in a cluster placement group. What is a key limitation of this configuration?

A. You cannot use Spot Instances in a cluster placement group.
B. You cannot launch new instances into the group if there is not enough available capacity on a single rack.
C. The network latency between instances will be high.
D. All instances in the group must be of the same size.

Answer: B

Explanation: A cluster placement group attempts to place all its instances on the same physical rack in a data center to minimize latency. The downside is that if that rack runs out of capacity for the instance type you need, your request to launch a new instance into the group will fail with an "insufficient capacity" error.

---

Question #: 32
What are the two Performance Modes available for Amazon EFS?

A. General Purpose and Provisioned IOPS
B. Standard and High Performance
C. General Purpose and Max I/O
D. Bursting and Provisioned

Answer: C

Explanation: Amazon EFS offers two performance modes to choose from when you create a file system: General Purpose (the default, optimized for latency) and Max I/O (optimized for aggregate throughput).

---

Question #: 33
An application needs to download a 10 GB file from S3 as quickly as possible to an EC2 instance. What can be done to maximize the download speed?

A. Enable S3 Transfer Acceleration on the bucket.
B. Use a ranged GET request to download multiple parts of the file in parallel.
C. Place the file in the S3 Intelligent-Tiering storage class.
D. Launch the EC2 instance in a different region from the S3 bucket.

Answer: B

Explanation: Similar to how Multipart Upload speeds up uploads, you can speed up downloads by making concurrent, ranged GET requests. Your application can issue multiple GET requests simultaneously, each for a different byte range of the object. It can then reassemble these parts on the EC2 instance. This parallelization can dramatically increase download throughput.

---

Question #: 34
Which EC2 instance family is designed for memory-intensive applications like in-memory databases (e.g., SAP HANA), real-time big data analytics, and large-scale enterprise applications?

A. C-family (Compute Optimized)
B. I-family (Storage Optimized)
C. M-family (General Purpose)
D. R-family or X-family (Memory Optimized)

Answer: D

Explanation: The R-family and X-family of instances are memory-optimized. They are designed to provide the lowest cost per GiB of RAM and are engineered for workloads that need to process large datasets in memory.

---

Question #: 35
What is the primary use case for an HDD-based EBS volume like st1 or sc1?

A. A boot volume for an EC2 instance.
B. A transactional database requiring high random IOPS.
C. Large-scale, sequential I/O workloads like data warehousing or log processing.
D. A shared file system for multiple EC2 instances.

Answer: C

Explanation: Hard Disk Drive (HDD) based volumes excel at providing high throughput for large, sequential read and write operations at a low cost. They are not well-suited for the small, random I/O patterns of a boot volume or a transactional database, which require the low latency of an SSD.

---

Question #: 36
You are designing a video-on-demand platform. The source video files are stored in an S3 bucket. How can you deliver this content to a global audience with low latency?

A. By using S3 Cross-Region Replication to copy the videos to all regions.
B. By using Amazon CloudFront with the S3 bucket as the origin.
C. By enabling S3 Transfer Acceleration on the bucket.
D. By hosting the files on an EFS file system instead.

Answer: B

Explanation: Amazon CloudFront is a Content Delivery Network (CDN). It caches copies of your content (like video files) in edge locations around the world. When a user requests a video, it is served from the edge location closest to them, which significantly reduces latency and improves the viewing experience. This is the standard architecture for delivering static/media content globally.

---

Question #: 37
An EBS volume is created from a snapshot. What is the performance characteristic of the volume immediately after creation?

A. It will have the maximum possible performance immediately.
B. The volume will experience higher latency for blocks that are being accessed for the first time as they are lazily loaded from S3.
C. The performance will be limited until the entire volume is read at least once.
D. The volume cannot be used until all data is restored from the snapshot.

Answer: B

Explanation: When you create a volume from a snapshot, the data is lazily loaded from S3 in the background. The volume is available for use immediately, but if your application accesses a block of data that hasn't been restored from S3 yet, it will experience a higher first-read latency. To avoid this, you can "pre-warm" the volume by reading all of its blocks, or use the Fast Snapshot Restore feature.

---

Question #: 38
What is the key benefit of using Amazon FSx for Lustre for a high-performance computing workload?

A. It provides a fully managed, low-cost NFS file system.
B. It provides a high-performance, parallel file system that can be tightly integrated with S3 for long-term storage.
C. It is designed for Windows-based applications.
D. It automatically encrypts all data in transit.

Answer: B

Explanation: FSx for Lustre is a specialized service that provides a file system optimized for high-performance, parallel I/O. A key feature is its seamless integration with Amazon S3. You can link your file system to an S3 bucket, allowing you to easily move your input data from S3 to the high-performance file system for processing, and then write the results back to S3 for durable, long-term storage.

---

Question #: 39
An EC2 instance needs to be able to sustain 10 Gbps of network bandwidth. Which of the following is a prerequisite?

A. The instance must be an EBS-optimized instance.
B. The instance must be launched within a VPC.
C. The instance must have Enhanced Networking enabled.
D. All of the above are required.

Answer: D

Explanation: To achieve high network performance like 10 Gbps, several conditions must be met. The instance must be launched in a VPC (all modern instances are). The instance type itself must support that level of performance. It must have Enhanced Networking (using the ENA driver) enabled. And for consistent high I/O to storage, being EBS-optimized is also a critical part of the overall performance picture.

---

Question #: 40
You have an EFS file system and you need to increase its throughput beyond what the Bursting Throughput mode can provide. What should you do?

A. Add more data to the file system.
B. Switch the file system to "Provisioned Throughput" mode and specify the desired throughput level.
C. Attach the file system to more EC2 instances.
D. Recreate the file system in a different Availability Zone.

Answer: B

Explanation: For workloads that require higher throughput than what is provided by the amount of data stored, you can switch to Provisioned Throughput mode. In this mode, you pay for and receive a consistent, specified level of throughput (in MiB/s) for your file system, regardless of its size.

---

Question #: 41
A workload requires a large number of EC2 instances to be launched in a single Availability Zone with the guarantee of minimal network latency between them. Which type of placement group should be used?

A. Spread
B. Partition
C. Cluster
D. Host

Answer: C

Explanation: A Cluster placement group is specifically designed for this purpose. It instructs AWS to place the instances as physically close together as possible on the same underlying hardware rack. This minimizes the network path between instances, resulting in the lowest possible inter-instance latency, which is critical for tightly coupled HPC applications.

---

Question #: 42
Which storage option provides the lowest latency access for an application running on an EC2 instance?

A. Amazon S3
B. An EBS gp3 volume
C. An EFS file system
D. A local NVMe SSD Instance Store

Answer: D

Explanation: The instance store consists of SSDs that are physically attached to the host computer that your EC2 instance is running on. Because the storage is directly attached, it bypasses the network and provides the absolute lowest latency and highest I/O performance possible. The trade-off is that this storage is ephemeral (non-persistent).

---

Question #: 43
When using a General Purpose SSD (gp3) EBS volume, what is the baseline IOPS performance that is included with the storage?

A. 1,000 IOPS
B. 3,000 IOPS
C. 5,000 IOPS
D. There is no baseline; all IOPS must be provisioned.

Answer: B

Explanation: A key feature of the gp3 volume type is that it includes a baseline performance of 3,000 IOPS and 125 MB/s of throughput, regardless of the volume's size. You can then provision additional IOPS (up to 16,000) and throughput (up to 1,000 MB/s) for an additional cost if your application requires it.

---

Question #: 44
A company needs to run a compute-intensive simulation that will take several hours. The simulation can be stopped and restarted without losing significant progress. To minimize costs, which EC2 purchasing model is most appropriate?

A. On-Demand
B. Reserved Instances
C. Spot Instances
D. Dedicated Hosts

Answer: C

Explanation: Spot Instances are ideal for workloads that are fault-tolerant and have flexible start and end times. Since the simulation can be interrupted and restarted, using Spot Instances will allow the company to run the compute job at a fraction of the On-Demand cost by leveraging AWS's spare capacity.

---

Question #: 45
What is the primary benefit of using an Amazon S3 VPC Gateway Endpoint?

A. It encrypts all S3 traffic.
B. It allows resources in your VPC to access S3 using private IP addresses without traversing the public internet.
C. It accelerates S3 uploads and downloads.
D. It allows you to mount an S3 bucket as a file system.

Answer: B

Explanation: A VPC Gateway Endpoint for S3 creates a private and secure connection between your VPC and the S3 service. It adds an entry to your route table that directs S3-bound traffic to the endpoint instead of an Internet Gateway. This enhances security by keeping traffic within the AWS network and can reduce data transfer costs.

---

Question #: 46
A company wants to place a group of EC2 instances in a way that reduces the risk of simultaneous failures. The instances are part of a critical application and should not share the same physical hardware rack. Which placement group strategy is suitable?

A. Cluster
B. Spread
C. Partition
D. None, this is not possible.

Answer: B

Explanation: A Spread placement group is designed for high availability of a small number of critical instances. It ensures that each instance is placed on a distinct hardware rack, each with its own network and power source. This minimizes the chance that a single hardware failure will take down more than one of your instances.

---

Question #: 47
Which EBS volume type offers the highest durability and is recommended for mission-critical applications like large relational databases?

A. General Purpose SSD (gp3)
B. Provisioned IOPS SSD (io2)
C. Throughput Optimized HDD (st1)
D. Cold HDD (sc1)

Answer: B

Explanation: Provisioned IOPS SSD (io1 and io2) volumes are designed with higher durability than General Purpose volumes. The io2 volume type, in particular, is designed for 99.999% durability (compared to 99.8% - 99.9% for gp3), making it the recommended choice for critical applications where data loss cannot be tolerated.

---

Question #: 48
You are designing an application that will be deployed on EC2 instances. The application requires a file system that can be accessed by both Linux and Windows instances simultaneously. Which AWS storage service should you use?

A. Amazon EFS
B. Amazon FSx for Windows File Server
C. Amazon FSx for Lustre
D. This is not possible.

Answer: B

Explanation: While EFS (A) is for Linux (using NFS) and FSx for Lustre (C) is a high-performance file system, Amazon FSx for Windows File Server uses the SMB protocol. The SMB protocol is natively supported by both Windows and modern Linux distributions (via clients like `cifs-utils`), making it the best choice for a shared file system that needs to be accessed by both operating systems.

---

Question #: 49
An application is performing a large number of small, random read and write operations on a storage volume. Which performance metric is most important for this workload?

A. Throughput (MB/s)
B. IOPS (Input/Output Operations Per Second)
C. Network Bandwidth (Gbps)
D. Durability (%)

Answer: B

Explanation: IOPS measures the number of read and write operations a storage device can perform per second. For workloads with many small, random I/O requests (like a transactional database), a high IOPS capability is the most critical performance indicator. Throughput (A) is more important for large, sequential file transfers.

---

Question #: 50
To achieve the highest possible throughput for a big data analytics job, you are using a fleet of I-family (storage optimized) EC2 instances. How can you aggregate the local NVMe instance stores of these instances into a single, high-performance file system?

A. This is not possible; instance stores cannot be combined.
B. By creating a RAID 0 array across the instance store volumes on a single instance.
C. By using a parallel file system like BeeGFS or Lustre, or a service like Amazon FSx for Lustre, which can be backed by the instances' local storage.
D. By using an EBS Multi-Attach volume.

Answer: C

Explanation: The NVMe drives in storage-optimized instances provide excellent performance. For a distributed job, you can use software to create a parallel file system that spans across the local disks of multiple instances. Services like FSx for Lustre can be configured to use S3 as a durable backend but present a high-performance file system to the compute fleet.

---

Question #: 51
You have an EFS file system that stores 100 GiB of data and is configured for Bursting Throughput. What is its baseline throughput?

A. 100 MiB/s
B. 50 MiB/s
C. 5 MiB/s
D. 1 MiB/s

Answer: C

Explanation: In Bursting Throughput mode, EFS provides a baseline throughput of 50 KiB/s per 1 GiB of data stored in the Standard storage class. Therefore, for 100 GiB of data, the baseline throughput would be 100 * 50 KiB/s = 5,000 KiB/s, which is approximately 5 MiB/s.

---

Question #: 52
What is the primary benefit of using an EC2 instance from an ARM-based AWS Graviton processor family (e.g., c6g, m6g) compared to a similar x86-based instance?

A. They offer better price-performance for many workloads.
B. They are compatible with all legacy software without modification.
C. They provide access to on-board FPGAs.
D. They have a higher maximum memory capacity.

Answer: A

Explanation: AWS Graviton processors are custom-built by AWS using the ARM architecture. They are designed to deliver significant price-performance benefits (up to 40% better in some cases) for a wide variety of cloud-native and scale-out workloads compared to equivalent x86-based instances from previous generations.

---

Question #: 53
Which S3 feature can automatically move objects between storage classes based on access patterns to optimize costs?

A. S3 Versioning
B. S3 Lifecycle Policies with Intelligent-Tiering
C. S3 Cross-Region Replication
D. S3 Batch Operations

Answer: B

Explanation: The S3 Intelligent-Tiering storage class is designed to automate cost savings. It works by monitoring the access patterns of your objects. If an object hasn't been accessed for a certain period (e.g., 30 days), Intelligent-Tiering will automatically move it to a lower-cost, infrequent access tier. If the object is accessed again, it is automatically moved back to the frequent access tier. S3 Lifecycle policies can also be used to transition objects, but Intelligent-Tiering is the automated, access-pattern-based solution.

---

Question #: 54
A company needs to launch a group of EC2 instances into a specific hardware rack that has unique features. They also need to ensure that no other AWS customer can run instances on that same physical server. Which EC2 tenancy option should they choose?

A. Default (shared) tenancy
B. Dedicated Instances
C. Dedicated Hosts
D. Spot Instances

Answer: C

Explanation: A Dedicated Host provides you with a physical server that is fully dedicated to your use. This is the highest level of isolation and is used for compliance requirements or software licenses that are tied to physical cores or sockets. You have visibility into the underlying hardware and can control instance placement. Dedicated Instances (B) run on dedicated hardware, but you don't have the same level of control over placement.

---

Question #: 55
What is the maximum amount of throughput you can provision for a single gp3 EBS volume?

A. 125 MB/s
B. 250 MB/s
C. 500 MB/s
D. 1,000 MB/s

Answer: D

Explanation: The gp3 volume type allows you to provision throughput independently of size. It includes a baseline of 125 MB/s, but you can provision additional throughput up to a maximum of 1,000 MB/s per volume.

---

Question #: 56
For which workload is an Amazon EFS file system a better choice than an Amazon EBS volume?

A. A boot volume for a single Windows EC2 instance.
B. A high-performance transactional database.
C. A shared content repository for a fleet of web servers running a content management system.
D. A volume requiring 100,000 IOPS.

Answer: C

Explanation: The key differentiator is the need for shared access. An EFS file system can be mounted and accessed by many EC2 instances simultaneously, making it perfect for use cases like a shared content repository, a home directory for multiple users, or a common data source for a cluster of application servers. EBS volumes are primarily for single-instance access.

---

Question #: 57
An application is experiencing high latency when reading data from an EBS volume. The CloudWatch metrics show that the volume's `BurstBalance` is at zero and the `VolumeQueueLength` is high. What does this indicate?

A. The network connection to the instance is saturated.
B. The EC2 instance's CPU is overloaded.
C. The application is demanding more IOPS than the EBS volume can provide, and it has exhausted its burst credits.
D. The EBS volume is not attached to an EBS-optimized instance.

Answer: C

Explanation: `BurstBalance` is the metric for burstable volumes (like gp2) that shows the remaining I/O credits. A balance of zero means the volume has been operating above its baseline performance and has used up all its credits, so its performance is now being throttled. A high `VolumeQueueLength` indicates that I/O operations are being queued up because the volume cannot service them fast enough. This confirms an I/O bottleneck at the volume level.

---

Question #: 58
You need to launch a group of EC2 instances for a tightly coupled workload. To reduce the impact of simultaneous hardware failures, you want to distribute the instances across a small number of distinct hardware racks within a single Availability Zone. Which placement group strategy is best?

A. Cluster
B. Spread
C. Partition
D. This is not possible.

Answer: C

Explanation: A Partition placement group provides a balance between the low latency of a cluster group and the high availability of a spread group. It divides the instances into logical partitions, and each partition resides on a separate hardware rack. This is ideal for distributed, partition-aware applications like HDFS, HBase, and Cassandra, as it limits the blast radius of a hardware failure to a single partition.

---

Question #: 59
What is a key performance benefit of using an Application Load Balancer instead of a Classic Load Balancer?

A. It supports routing based on the content of the request, such as the URL path, which can direct traffic to optimized microservices.
B. It provides a static IP address for each Availability Zone.
C. It can handle non-HTTP protocols.
D. It has a lower cost for all traffic levels.

Answer: A

Explanation: A major advantage of the ALB is its ability to perform content-based routing at Layer 7. This allows you to have a single load balancer that serves multiple microservices. For example, requests to `/api` go to the API servers, and requests to `/images` go to the image processing servers. This simplifies the architecture and allows for more efficient resource utilization compared to the simple routing of a Classic Load Balancer.

---

Question #: 60
An application requires a storage volume that can provide a sustained 20,000 IOPS. The data size is 1 TB. Which EBS volume type should be selected?

A. General Purpose SSD (gp3)
B. Throughput Optimized HDD (st1)
C. Provisioned IOPS SSD (io2)
D. Cold HDD (sc1)

Answer: C

Explanation: The requirement for 20,000 sustained IOPS is beyond the maximum capability of a gp3 volume (16,000 IOPS). Therefore, a Provisioned IOPS SSD volume (io1 or io2) is required. An io2 volume can be provisioned with the necessary 20,000 IOPS to meet the application's performance demands consistently.

---

Question #: 61
You need to run a graphical workstation in the cloud for video rendering. The application requires a powerful GPU. Which EC2 instance family should you choose?

A. C-family (Compute Optimized)
B. R-family (Memory Optimized)
C. G-family or P-family (Accelerated Computing)
D. T-family (Burstable)

Answer: C

Explanation: The G-family and P-family of instances are part of the Accelerated Computing category. They are equipped with powerful NVIDIA GPUs and are designed for graphics-intensive applications like 3D visualizations and video rendering, as well as for machine learning and high-performance computing.

---

Question #: 62
When uploading a 50 GB file to S3, which practice is recommended to improve performance and reliability?

A. Use a single PUT operation for the entire file.
B. Compress the file before uploading.
C. Use Multipart Upload.
D. Use a Spot EC2 instance for the upload.

Answer: C

Explanation: For any file larger than 100 MB (and required for files > 5 GB), you should use Multipart Upload. This process breaks the file into smaller, independent parts that can be uploaded in parallel. This improves throughput by maximizing bandwidth usage and increases reliability because a failure of a single part only requires that part to be re-transmitted, not the entire file.

---

Question #: 63
An EFS file system is used by a web application to store user-uploaded images. The number of images is growing rapidly. How does the EFS file system scale to handle this growth?

A. You must manually provision a larger file system size.
B. EFS automatically grows and shrinks as you add and remove files, with no need for manual provisioning.
C. You must add more EFS mount targets.
D. You need to attach a larger EBS volume to the file system.

Answer: B

Explanation: A key feature of Amazon EFS is its elasticity. The file system storage capacity is fully elastic, meaning it scales up or down automatically as you add or remove files. You are billed only for the amount of storage you are actually using.

---

Question #: 64
An application requires the lowest possible latency for storage I/O. Data persistence is not a concern, as the data is temporary. Which storage option should be used?

A. EBS Provisioned IOPS SSD (io2)
B. EBS General Purpose SSD (gp3)
C. EFS in General Purpose mode
D. Local NVMe SSD Instance Store

Answer: D

Explanation: For the absolute lowest latency, nothing beats a local instance store. Because the SSDs are physically connected to the host server, the data does not have to traverse the network like it does for EBS or EFS. This provides microsecond-level latency, but with the trade-off that the data is ephemeral and will be lost if the instance is stopped or terminated.

---

Question #: 65
What is the primary factor that determines the network performance of an EC2 instance?

A. The AWS Region it is in.
B. The instance type and size.
C. The EBS volume type attached to it.
D. The number of security groups attached.

Answer: B

Explanation: Network performance (bandwidth, packets per second) is a characteristic of the EC2 instance type. In general, larger instances within a family have better network performance than smaller instances. AWS publishes network performance ratings like "Up to 5 Gbps" or "25 Gbps" for its various instance types.

---

Question #: 66
You are trying to attach a single EBS volume to two different EC2 instances at the same time. The operation fails. What is the most likely reason?

A. The instances are in different Availability Zones.
B. Standard EBS volumes do not support being attached to multiple instances simultaneously.
C. The EBS volume is not encrypted.
D. The EC2 instances are not part of a cluster placement group.

Answer: B

Explanation: By default, an EBS volume can only be attached to one EC2 instance at a time in the same Availability Zone. While there is a feature called EBS Multi-Attach, it is only supported for Provisioned IOPS volumes (io1/io2) and requires a clustered file system to manage concurrent access safely. For the vast majority of use cases, the one-to-one relationship holds true.

---

Question #: 67
To optimize the cost of an EFS file system that contains a mix of frequently and infrequently accessed files, what feature should be used?

A. EFS Provisioned Throughput
B. EFS Lifecycle Management
C. EFS Replication
D. EFS Backup

Answer: B

Explanation: EFS Lifecycle Management helps you save money by automatically moving files that have not been accessed for a specified period of time from the EFS Standard storage class to the lower-cost EFS Infrequent Access (IA) storage class. If a file in IA is accessed again, it is automatically moved back to Standard.

---

Question #: 68
What is the maximum size of a single object that can be stored in Amazon S3?

A. 5 GB
B. 100 GB
C. 5 TB
D. There is no limit.

Answer: C

Explanation: While the total amount of data you can store in S3 is virtually unlimited, there is a limit on the size of a single object. The maximum size for one object in S3 is 5 terabytes (TB).

---

Question #: 69
A database requires a storage volume that can provide at least 15,000 IOPS. You are using a gp3 volume. What is a valid configuration?

A. A 1000 GiB gp3 volume with 15,000 IOPS provisioned.
B. A 5000 GiB gp2 volume.
C. A 1000 GiB gp3 volume with 20,000 IOPS provisioned.
D. This is not possible with gp3.

Answer: A

Explanation: The gp3 volume type supports a maximum of 16,000 IOPS. Therefore, provisioning a volume with 15,000 IOPS is a valid configuration. To get 15,000 IOPS from a gp2 volume (B), you would need 5000 GiB (at 3 IOPS/GiB), which is much more expensive. 20,000 IOPS (C) is beyond the limit for gp3.

---

Question #: 70
Which EC2 instance feature allows an application to directly access the memory of the network adapter, bypassing the operating system kernel, to reduce latency?

A. Enhanced Networking
B. Elastic IP Address
C. Elastic Fabric Adapter (EFA)
D. EBS Optimization

Answer: C

Explanation: The Elastic Fabric Adapter (EFA) uses a technique called OS-bypass. This allows supported applications (typically those using MPI or NCCL libraries) to communicate directly with the network hardware, bypassing the OS kernel's network stack. This significantly reduces latency and improves inter-node communication performance, which is critical for tightly coupled HPC and ML workloads.

---

Question #: 71
When should you choose an HDD-based EBS volume (st1 or sc1) over an SSD-based volume (gp3 or io2)?

A. When you need a boot volume for an instance.
B. When the workload is characterized by large, sequential I/O operations and low cost is a priority.
C. When you need the lowest possible latency for random I/O.
D. When you need to attach the volume to multiple instances.

Answer: B

Explanation: HDD-based volumes are optimized for throughput, not IOPS. They are ideal for workloads that read or write large, contiguous blocks of data, such as streaming, log processing, or data warehousing. For these use cases, they provide good performance at a much lower price point than SSD-based volumes.

---

Question #: 72
An S3 bucket is used to store millions of log files. A data analytics job needs to read all the logs from a specific day. The objects are named with the following convention: `YYYY-MM-DD-hh-mm-ss-log.txt`. How can you optimize the S3 GET request performance for this job?

A. Use a more random naming convention for the object keys.
B. Enable versioning on the S3 bucket.
C. Store the logs in a single, large compressed file instead of many small files.
D. The performance will be optimal by default.

Answer: A

Explanation: Naming S3 objects with sequential, date-based prefixes can sometimes lead to performance hot-spotting, as all the write (or read) activity is concentrated on a single partition within the S3 index. By introducing some randomness to the beginning of the object key (e.g., a hashed value prefix like `A1B2-YYYY-MM-DD-...`), you can distribute the requests across multiple partitions, allowing S3 to scale its performance more effectively.

---

Question #: 73
What is the primary use case for Amazon EC2 instance store?

A. Long-term, durable storage for databases.
B. A shared file system for a cluster of instances.
C. Temporary storage for caches, buffers, or scratch space for data that is replicated elsewhere.
D. A boot volume for an EC2 instance.

Answer: C

Explanation: Because instance store is ephemeral (non-persistent), it should never be used for data that needs to be durable. However, its extremely high performance and low latency make it an excellent choice for temporary storage. Common use cases include caches for web servers, buffer space for data processing jobs, or scratch disks for applications where the final results are written to a persistent store like S3 or EBS.

---

Question #: 74
A company needs to launch several EC2 instances for a high-performance, tightly coupled scientific computing application. The application requires the lowest possible inter-node latency. Which placement group strategy is required?

A. Spread
B. Cluster
C. Partition
D. Any placement group will provide low latency.

Answer: B

Explanation: A Cluster placement group is the only strategy that is explicitly designed to minimize network latency between instances. It achieves this by placing the instances on the same physical rack within a single Availability Zone, ensuring the shortest possible network path between them.

---

Question #: 75
What are two key characteristics of the General Purpose SSD (gp3) EBS volume type? (Choose TWO)

A. It is an HDD-based volume.
B. It allows you to provision IOPS and throughput independently from storage size.
C. It is designed for the highest possible durability of 99.999%.
D. It provides a baseline of 3,000 IOPS and 125 MB/s of throughput included with the price of storage.
E. It can be attached to multiple EC2 instances simultaneously by default.

Answer: B, D

Explanation: The defining features of gp3 are its flexibility and baseline performance. It decouples performance from size (B), allowing you to configure the IOPS and throughput you need. It also comes with a generous baseline performance (D) that is sufficient for a wide range of applications, making it a very cost-effective and flexible choice.

---

Question #: 76
To get the highest network throughput between an EC2 instance and an S3 bucket in the same region, what should be done?

A. Use an S3 VPC Interface Endpoint.
B. Launch the EC2 instance in a cluster placement group.
C. Use the AWS CLI with parallel uploads/downloads enabled by default, or use a tool that supports concurrent requests.
D. Use S3 Transfer Acceleration.

Answer: C

Explanation: S3 can handle a massive amount of throughput. The bottleneck is often the client's ability to send a single stream of data fast enough. By using a tool that can make multiple concurrent connections to S3 (like the AWS CLI for large files, or custom SDK code), you can parallelize the data transfer and better utilize the available network bandwidth of your EC2 instance and S3's scalable front-end.

---

Question #: 77
Which EC2 instance type is designed as a balance of compute, memory, and networking resources, making it suitable for a wide variety of diverse workloads?

A. Compute Optimized (C-family)
B. Storage Optimized (I-family)
C. Memory Optimized (R-family)
D. General Purpose (M-family or T-family)

Answer: D

Explanation: The General Purpose instance families, such as the M-family (e.g., m5, m6g) and the T-family (e.g., t3, t4g), are engineered to provide a balanced combination of resources. They are the workhorses of EC2 and are a good starting point for many different applications, including web servers, small-to-mid-sized databases, and development environments.

---

Question #: 78
An application uses an EFS file system and is experiencing performance issues. The CloudWatch metrics show a high `PercentIOLimit`. What does this indicate?

A. The file system is running out of storage space.
B. The EC2 instances are not able to connect to the file system.
C. The file system is operating at its maximum IOPS limit for an extended period.
D. The file system is in the wrong Availability Zone.

Answer: C

Explanation: The `PercentIOLimit` metric for EFS indicates how close the file system is to its IOPS limit, which is based on the performance mode and size. A consistently high value for this metric (near 100%) means the file system is being throttled because the application is driving more I/O operations than the file system can handle. This would suggest a need to switch to Max I/O mode or use a different storage solution if the workload is not a good fit for EFS.

---

Question #: 79
What is a primary advantage of using an EBS-optimized instance for an I/O-intensive application?

A. It reduces the cost of EBS snapshots.
B. It provides a dedicated network path for EBS traffic, minimizing contention with other network traffic from the instance.
C. It allows you to attach more EBS volumes than a standard instance.
D. It automatically selects the best EBS volume type for your workload.

Answer: B

Explanation: On non-optimized instances, the traffic to and from EBS volumes shares the same network interface as all other network traffic (e.g., to the internet or other instances). On an EBS-optimized instance, there is a dedicated, separate bandwidth allocation just for EBS traffic. This prevents your storage performance from being impacted by a burst of network traffic, and vice-versa, ensuring consistent I/O.

---

Question #: 80
Which S3 feature would you use to automatically create a copy of all newly uploaded objects into a bucket in another AWS region for disaster recovery?

A. S3 Versioning
B. S3 Lifecycle Policies
C. S3 Cross-Region Replication (CRR)
D. S3 Batch Operations

Answer: C

Explanation: S3 Cross-Region Replication is the service feature specifically designed for this purpose. You can configure a replication rule on a source bucket that tells S3 to automatically and asynchronously copy all new objects (or objects matching a certain prefix) to a specified destination bucket in a different AWS region.

---

Question #: 81
You need to create a temporary file system on an EC2 instance for scratch data. The data needs to be accessed at the highest possible speed. Which storage should you use?

A. An EBS Cold HDD (sc1) volume.
B. An EFS file system in Max I/O mode.
C. The instance's local NVMe instance store.
D. An EBS General Purpose (gp3) volume.

Answer: C

Explanation: For temporary, high-speed scratch space, the instance store is the ideal choice. It is a physically attached SSD that offers the lowest latency and highest IOPS of any storage option available to an EC2 instance. Its ephemeral nature is acceptable because the data is only needed temporarily.

---

Question #: 82
A workload requires a shared file system that is accessible from thousands of EC2 instances and scales its throughput as the amount of data grows. Which service is designed for this elastic scaling?

A. Amazon EBS
B. Amazon FSx for Lustre
C. Amazon S3
D. Amazon EFS

Answer: D

Explanation: Amazon EFS is designed for exactly this type of elastic, scalable, shared access. Its performance and capacity scale automatically as your needs change. It can support thousands of concurrent connections and is a fully managed service, making it easy to deploy and use for scale-out applications.

---

Question #: 83
What is the key difference in performance characteristics between an st1 and an io2 EBS volume?

A. st1 is SSD-based, while io2 is HDD-based.
B. st1 is optimized for high sequential throughput (MB/s), while io2 is optimized for high random IOPS.
C. st1 has higher durability than io2.
D. st1 can be attached to multiple instances, while io2 cannot.

Answer: B

Explanation: This highlights the core difference between throughput-optimized and IOPS-optimized volumes. st1 (an HDD volume) is designed to perform well when reading or writing large, sequential blocks of data, measuring its performance in MB/s. io2 (an SSD volume) is designed to perform well with small, random reads and writes, measuring its performance in IOPS.

---

Question #: 84
To achieve the best S3 performance for an application that frequently uploads and downloads many small objects, what is a key architectural consideration?

A. Use a single, sequential process to handle all S3 requests.
B. Design the application to make multiple, parallel requests to S3 simultaneously.
C. Compress all the small objects into a single large archive before uploading.
D. Use the S3 Standard-IA storage class.

Answer: B

Explanation: S3 is a massively parallel, distributed system. To leverage its scale, your application should be designed to interact with it in parallel. By using multiple threads or an asynchronous I/O model to make many requests at once, you can achieve a much higher total throughput than by making one request at a time.

---

Question #: 85
Which of the following EC2 features is designed to provide fault tolerance for a distributed application by spreading instances across distinct hardware?

A. Elastic Fabric Adapter (EFA)
B. Partition Placement Group
C. EBS Optimization
D. Burstable Performance

Answer: B

Explanation: A Partition placement group is a high-availability feature. It divides your instances into logical partitions (with each partition on a separate physical rack). This ensures that a hardware failure on a single rack will only affect the instances within one partition, limiting the "blast radius" of the failure for your application.

---

Question #: 86
You need to run a high-performance database on an EC2 instance. The database requires at least 50,000 IOPS. Which EBS volume type can meet this requirement?

A. gp2
B. gp3
C. st1
D. io2

Answer: D

Explanation: The maximum IOPS for a gp3 volume is 16,000. To achieve 50,000 IOPS, you must use a Provisioned IOPS SSD volume. The io1 and io2 volume types can be provisioned with up to 64,000 IOPS (and io2 Block Express up to 256,000 IOPS), making them the correct choice for this high-performance requirement.

---

Question #: 87
An application uses Amazon EFS. To lower costs, the company wants to move files that haven't been accessed in 30 days to a cheaper storage tier. How can this be automated?

A. By enabling EFS Intelligent-Tiering.
B. By creating an EFS Lifecycle policy.
C. By writing a Lambda function to manually move the files.
D. This is not possible with EFS.

Answer: B

Explanation: EFS Lifecycle Management is the feature that automates this process. You can create a lifecycle policy that specifies a time period (e.g., 30 days of inactivity). EFS will then automatically and transparently move any file that hasn't been accessed within that period from the EFS Standard tier to the lower-cost EFS Infrequent Access (IA) tier.

---

Question #: 88
A team is building a solution that requires a high-performance, shared file system for a Linux-based HPC cluster. The main priority is processing speed. Which AWS service is purpose-built for this?

A. Amazon S3 with a file gateway
B. Amazon EFS in General Purpose mode
C. Amazon FSx for Lustre
D. Amazon EBS with Multi-Attach

Answer: C

Explanation: FSx for Lustre is a service that provides a file system designed for high-performance computing. Lustre is a popular open-source parallel file system, and FSx provides a fully managed version of it, offering extremely high throughput and low latencies that are ideal for HPC workloads.

---

Question #: 89
What happens to the data on an EBS volume when the EC2 instance it is attached to is terminated?

A. The EBS volume is always deleted along with the instance.
B. The EBS volume is always detached and preserved.
C. By default, the root EBS volume is deleted, while any non-root volumes are preserved. This behavior can be changed.
D. The data is automatically moved to an S3 bucket.

Answer: C

Explanation: The behavior depends on the "Delete on Termination" flag for the volume attachment. For the root volume of an instance, this flag is set to "true" by default. For any other (non-root) data volumes that you attach, the flag is set to "false" by default. You can change this setting for any volume.

---

Question #: 90
What is a key performance difference between an S3 bucket and an EFS file system?

A. S3 offers massively scalable throughput and request rates for objects, while EFS offers low-latency, POSIX-compliant file system access.
B. S3 is faster for small, random file access.
C. EFS has higher durability than S3.
D. EFS can store larger individual files than S3.

Answer: A

Explanation: They are optimized for different access patterns. S3 is an object store built for massive parallelism, making it ideal for high-throughput streaming of large objects or handling millions of concurrent requests for smaller objects. EFS is a file system that provides the semantics and low-latency metadata operations (like listing directories, renaming files) that are expected from a traditional network file system.

---

Question #: 91
You need to launch a memory-intensive application that processes large datasets in RAM. Which EC2 instance family should you primarily consider?

A. T-family
B. C-family
C. R-family
D. I-family

Answer: C

Explanation: The R-family of instances is memory-optimized. They have the highest ratio of RAM to vCPU, making them the ideal choice for workloads like in-memory databases, large-scale caching, and real-time big data analytics that require large amounts of memory.

---

Question #: 92
A gp3 EBS volume is configured with 500 GiB of storage and 500 MB/s of throughput. What is its IOPS capability?

A. 3,000 IOPS
B. 5,000 IOPS
C. 10,000 IOPS
D. It must be explicitly provisioned.

Answer: A

Explanation: A gp3 volume includes a baseline of 3,000 IOPS and 125 MB/s of throughput regardless of its size. You have provisioned additional throughput, but since you have not explicitly provisioned additional IOPS, the volume will have the baseline of 3,000 IOPS.

---

Question #: 93
What is the purpose of an S3 bucket's "prefix" in relation to performance?

A. It encrypts the objects stored under that prefix.
B. It defines the storage class for the objects.
C. It acts as a partitioning key. S3 can scale to support very high request rates for each unique prefix.
D. It is a tag used for billing purposes.

Answer: C

Explanation: S3 achieves its massive scale by partitioning its index based on object prefixes. Each prefix can scale to handle thousands of requests per second. Therefore, by distributing your objects and requests across multiple prefixes, you can achieve a much higher aggregate request rate than if you stored all objects under a single prefix.

---

Question #: 94
An application requires a minimum of 400 MB/s of sequential throughput for processing large log files. The data size is 1 TB. Which EBS volume is the most cost-effective option?

A. A 1 TB gp3 volume with 400 MB/s provisioned.
B. A 1 TB st1 volume.
C. A 1 TB io2 volume with 10,000 IOPS.
D. A 400 GB sc1 volume.

Answer: B

Explanation: The workload is for large, sequential throughput, which is the exact use case for the Throughput Optimized HDD (st1) volume type. A 1 TB st1 volume can burst to 500 MB/s and has a baseline of 40 MB/s per TB, so it easily meets the 400 MB/s requirement. It will be significantly cheaper than a gp3 or io2 volume configured for the same throughput.

---

Question #: 95
A company is running a tightly coupled molecular modeling simulation on a large cluster of EC2 instances. Which combination of features would provide the best performance?

A. A spread placement group and an Application Load Balancer.
B. A partition placement group and EFS storage.
C. A cluster placement group and an Elastic Fabric Adapter (EFA).
D. A Dedicated Host and EBS Multi-Attach.

Answer: C

Explanation: For tightly coupled HPC workloads, you need the lowest possible latency and highest possible bandwidth between nodes. A Cluster placement group places the instances physically close together. An Elastic Fabric Adapter (EFA) provides a specialized network interface that bypasses the OS kernel to provide extremely low-latency communication, which is ideal for the MPI communication patterns used in these simulations.

---

Question #: 96
Which of the following is a key characteristic of Amazon S3 storage?

A. It is a block storage service.
B. It is a file storage service.
C. It is an object storage service with a flat namespace.
D. It is an ephemeral storage service.

Answer: C

Explanation: S3 is an object storage service. This means it stores data as objects within buckets. Each object consists of the data itself, a unique key (its name), and metadata. Unlike a file system, it has a flat structure (though the use of prefixes in keys can simulate a hierarchy).

---

Question #: 97
An EC2 instance is performing poorly. CloudWatch metrics show 100% CPU utilization and a high `DiskReadOps` metric for its EBS volume. What is the most likely bottleneck?

A. The instance is network-bound.
B. The instance is CPU-bound and potentially I/O-bound.
C. The EBS volume is configured with too much throughput.
D. The instance is memory-bound.

Answer: B

Explanation: 100% CPU utilization clearly indicates that the instance is CPU-bound; it does not have enough processing power for its workload. A high `DiskReadOps` (IOPS) metric suggests that it is also heavily using its storage. This combination means the instance is likely bottlenecked on both CPU and I/O, and a larger instance type or a volume with higher IOPS may be needed.

---

Question #: 98
When is it appropriate to use an EFS file system in "Max I/O" performance mode?

A. For a latency-sensitive web content management system.
B. For a large-scale, parallel data processing job that needs to maximize aggregate throughput.
C. When you need to provision a specific level of throughput.
D. When cost is the most important factor.

Answer: B

Explanation: Max I/O mode is designed for applications that are highly parallel and need to achieve the highest possible aggregate throughput and IOPS from the file system. It trades a small amount of metadata latency for this increased scale, making it ideal for big data analytics, media processing, and large-scale simulations.

---

Question #: 99
You are using an EBS Provisioned IOPS (io2) volume. What is the maximum ratio of provisioned IOPS to requested volume size (in GiB)?

A. 100:1
B. 500:1
C. 1,000:1
D. 3,000:1

Answer: C

Explanation: For io1 and io2 volumes, there is a maximum ratio of IOPS to size that you can provision. For io2 volumes, this ratio is 1,000 IOPS per GiB. This means, for example, a 10 GiB io2 volume can be provisioned with a maximum of 10,000 IOPS.

---

Question #: 100
A web application serves a large amount of static content (images, CSS, JavaScript) from an EFS file system mounted across a fleet of EC2 web servers. Users are reporting slow page load times. What is the BEST way to improve the performance for a global user base?

A. Switch the EFS performance mode to Max I/O.
B. Increase the size of the EC2 instances.
C. Use Amazon CloudFront as a CDN to cache the static content at edge locations closer to the users.
D. Increase the Provisioned Throughput of the EFS file system.

Answer: C

Explanation: The best way to improve performance for delivering static content to a global audience is to use a Content Delivery Network (CDN) like Amazon CloudFront. CloudFront will cache the static assets at its numerous edge locations around the world. When a user requests an asset, it will be served from a nearby edge location instead of making a long trip back to your EFS file system, dramatically reducing latency and improving performance.
